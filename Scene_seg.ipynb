{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Scene_seg_venkatsai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRwJQTbnizkD",
        "outputId": "e174235c-95fe-4200-9977-bbd8f5540d6c"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "from torch.utils import data\r\n",
        "import glob\r\n",
        "from collections import Counter\r\n",
        "!pip3 install pickle5\r\n",
        "import pickle5 as pickle\r\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f29a44d18f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvzI8aGr935E"
      },
      "source": [
        "# Data Loading Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hwL6u4oQBVx",
        "outputId": "24518f76-5e93-4f6a-eabb-0f33bdce3bc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asPIpV1AoQYz"
      },
      "source": [
        "def get_data(path):\r\n",
        "    train_X = []\r\n",
        "    train_Y = []\r\n",
        "    train_lengths = []\r\n",
        "\r\n",
        "    val_X = []\r\n",
        "    val_Y = []\r\n",
        "    val_lengths = []\r\n",
        "\r\n",
        "    test_X = []\r\n",
        "    test_Y = []\r\n",
        "    test_lengths = []\r\n",
        "\r\n",
        "    for e, file in enumerate(sorted(glob.glob(path + '/*.pkl'))):\r\n",
        "        with open(f'{file}', 'rb') as f:\r\n",
        "            data = pickle.load(f)\r\n",
        "        att = (data['place'], data['cast'], data['action'], data['audio'])\r\n",
        "        att = torch.cat(att, dim=1)\r\n",
        "        target = torch.cat((data['scene_transition_boundary_ground_truth'].long(),\r\n",
        "                            torch.tensor([0])), axis=0)\r\n",
        "        \r\n",
        "        assert(len(target) == len(att))\r\n",
        "        if e <= 49:\r\n",
        "            train_X.append(att)\r\n",
        "            train_Y.append(target)\r\n",
        "            train_lengths.append(len(data['place']))\r\n",
        "        elif 50 <= e <= 56:\r\n",
        "            val_X.append(att)\r\n",
        "            val_Y.append(target)\r\n",
        "            val_lengths.append(len(data['place']))\r\n",
        "        else:\r\n",
        "            test_X.append(att)\r\n",
        "            test_Y.append(target)\r\n",
        "            test_lengths.append(len(data['place']))\r\n",
        "    \r\n",
        "    return((train_X, train_Y, train_lengths), (val_X, val_Y, val_lengths), (test_X, test_Y, test_lengths))"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ht6IPAokMv"
      },
      "source": [
        "# PATHS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VoV0yleojjk"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/eluvio_data/data'\r\n",
        "model_path = '/content/drive/MyDrive/eluvio_data/saved_models/model_new6.pt'\r\n",
        "dump_path = '/content/drive/MyDrive/eluvio_data/data_pred/'"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0_Ku9yOskpI"
      },
      "source": [
        "# Train, Validation, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWRYibVKqhho"
      },
      "source": [
        "train_data, val_data, test_data = get_data(data_path)\r\n",
        "\r\n",
        "trainX, trainY, train_movie_lengths = train_data\r\n",
        "valX, valY, val_movie_lengths = val_data\r\n",
        "testX, testY, test_movie_lengths = test_data"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boaOHwS_7-jg"
      },
      "source": [
        "# Hyperparameters and Parameters\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkh0_Bchn3Nw"
      },
      "source": [
        "sequence_length = 20\r\n",
        "batch_size = 8\r\n",
        "epochs = 20"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgNJjWODnzLN"
      },
      "source": [
        "num_context_shots_before = 1\r\n",
        "num_context_shots_after = 2\r\n",
        "new_sequence_length = sequence_length + num_context_shots_before + num_context_shots_after\r\n",
        "num_context_shots = num_context_shots_before + num_context_shots_after + 1"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPMhnbIzn0Bq"
      },
      "source": [
        "place_feat_dim = 2048\r\n",
        "other_feat_dim = 512\r\n",
        "feat_dim = place_feat_dim + (3 * other_feat_dim)\r\n",
        "\r\n",
        "load_pretrained_model = True\r\n",
        "num_out_channels = 512"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sfXXaVBi5Uv",
        "outputId": "000e9a61-28ec-4347-89ce-940b694fdfa8"
      },
      "source": [
        "ctx = np.arange(-num_context_shots_before,\r\n",
        "                sequence_length + num_context_shots_after)\r\n",
        "ctx"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
              "       16, 17, 18, 19, 20, 21])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZvl4i9DsqHW"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTV-y1Mg9hUQ"
      },
      "source": [
        "def get_padded_tensors_idx(X, Y, test_mode=False):\r\n",
        "    idx = []\r\n",
        "    start = 0\r\n",
        "\r\n",
        "    if test_mode is False:\r\n",
        "        for i in range(len(X)):\r\n",
        "            rem_len = len(X[i]) % sequence_length\r\n",
        "            if rem_len < num_context_shots_after:\r\n",
        "                X[i] = F.pad(X[i], (0, 0, 0, num_context_shots_before + num_context_shots_after - rem_len),\r\n",
        "                            mode='constant', value=0)\r\n",
        "                Y[i] = F.pad(Y[i], (0, num_context_shots_before + num_context_shots_after - rem_len),\r\n",
        "                            mode='constant', value=-100)\r\n",
        "            else:\r\n",
        "                X[i] = X[i][:len(X[i]) - rem_len + num_context_shots_after]\r\n",
        "                Y[i] = Y[i][:len(Y[i]) - rem_len + num_context_shots_after]\r\n",
        "\r\n",
        "                X[i] = F.pad(X[i], (0, 0, 0, num_context_shots_before),\r\n",
        "                            mode='constant', value=0)\r\n",
        "                Y[i] = F.pad(Y[i], (0, num_context_shots_before),\r\n",
        "                            mode='constant', value=-100)\r\n",
        "\r\n",
        "            end = start + len(X[i]) - \\\r\n",
        "                (num_context_shots_before + num_context_shots_after)\r\n",
        "            idx.extend(np.arange(start, end, sequence_length))\r\n",
        "            start = end + (num_context_shots_before + num_context_shots_after)\r\n",
        "    else:\r\n",
        "        pad_spec_X = (0, 0, 0, new_sequence_length)\r\n",
        "        pad_spec_Y = (0, new_sequence_length)\r\n",
        "        start = 0\r\n",
        "        for i in range(len(X)):\r\n",
        "            padded_data = F.pad(X[i], pad_spec_X,\r\n",
        "                                mode='constant', value=0)\r\n",
        "            X[i] = padded_data\r\n",
        "            Y[i] = F.pad(Y[i], pad_spec_Y, mode='constant', value=-100)\r\n",
        "            end = start + len(padded_data) - new_sequence_length\r\n",
        "            idx.extend(np.arange(start, end, step=sequence_length))\r\n",
        "            start = end + new_sequence_length\r\n",
        "\r\n",
        "\r\n",
        "    X = torch.cat((X))\r\n",
        "    Y = torch.cat((Y))\r\n",
        "\r\n",
        "    X = F.pad(X, (0, 0, num_context_shots_before, 0),\r\n",
        "                mode='constant', value=0)\r\n",
        "    Y = F.pad(Y, (num_context_shots_before, 0),\r\n",
        "                mode='constant', value=-100)\r\n",
        "    idx = np.array(idx) + num_context_shots_before\r\n",
        "\r\n",
        "    return(X, Y, idx)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpwTRwcQi5Lu"
      },
      "source": [
        "trainX, trainY, train_idx = get_padded_tensors_idx(trainX, trainY)\r\n",
        "valX, valY, val_idx = get_padded_tensors_idx(valX, valY)\r\n",
        "testX, testY, test_idx = get_padded_tensors_idx(testX, testY, test_mode=True)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbDKSfbEVQK"
      },
      "source": [
        "# Data Set and Sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYHPcmksi5Jj"
      },
      "source": [
        "class MovieDataset(data.Dataset):\r\n",
        "    def __init__(self, X, Y, length_data):\r\n",
        "        self.X = X\r\n",
        "        self.Y = Y\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.len = length_data\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        index = np.expand_dims(index, axis=1)\r\n",
        "        seq_idx = index + ctx\r\n",
        "        label_idx = (index + np.arange(sequence_length)).flatten()\r\n",
        "        X_batched = self.X[seq_idx, :].reshape(-1,\r\n",
        "                                               new_sequence_length,\r\n",
        "                                               feat_dim)\r\n",
        "        Y_batched = self.Y[label_idx]\r\n",
        "        return(X_batched, Y_batched)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return (self.len)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaIj9_jai5HP"
      },
      "source": [
        "class RandomBatchSampler(data.sampler.Sampler):\r\n",
        "    def __init__(self, batch_size, ids, test_mode=False):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.ids = ids\r\n",
        "        self.length = self.ids.shape[0]\r\n",
        "        self.test_mode = test_mode\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        if self.test_mode is False:\r\n",
        "            inter = torch.randperm(self.ids.shape[0]).tolist()\r\n",
        "            rand_idx = self.ids[inter]\r\n",
        "        else:\r\n",
        "            rand_idx = self.ids\r\n",
        "        data_iter = iter([rand_idx[i:i + self.batch_size]\r\n",
        "                          for i in range(0, len(rand_idx), self.batch_size)])\r\n",
        "        return data_iter\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return (self.length // self.batch_size)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqX_31UpCI5s"
      },
      "source": [
        "# Train  Dataloader and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h8r8Ttli5E5"
      },
      "source": [
        "sampler_train = RandomBatchSampler(batch_size=batch_size, ids=train_idx)\r\n",
        "train_set = MovieDataset(trainX, trainY, len(train_idx))\r\n",
        "train_loader = data.DataLoader(train_set,\r\n",
        "                               batch_size=1,\r\n",
        "                               num_workers=2,\r\n",
        "                               sampler=sampler_train,\r\n",
        ")"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szu8pVSFCatz"
      },
      "source": [
        "# Val  Dataloader and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5eDVA45CHaH"
      },
      "source": [
        "sampler_val = RandomBatchSampler(batch_size=batch_size, ids=val_idx)\r\n",
        "val_set = MovieDataset(valX, valY, len(val_idx))\r\n",
        "val_loader = data.DataLoader(val_set,\r\n",
        "                            batch_size=1,\r\n",
        "                            num_workers=2,\r\n",
        "                            sampler=sampler_val,\r\n",
        "                            pin_memory=True\r\n",
        ")"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MbLYmQeDIiu"
      },
      "source": [
        "# Test Dataloader and Datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqcsuw30DH_E"
      },
      "source": [
        "sampler_test = RandomBatchSampler(batch_size=1, ids=test_idx, test_mode=True)\r\n",
        "test_set = MovieDataset(testX, testY, len(test_idx))\r\n",
        "test_loader = data.DataLoader(test_set,\r\n",
        "                              batch_size=1,\r\n",
        "                              num_workers=2,\r\n",
        "                              sampler=sampler_test,\r\n",
        "                              pin_memory=True\r\n",
        "                             )"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDzWElYoJKi1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL_GtZlFYEzk"
      },
      "source": [
        "class Cos(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Cos, self).__init__()\r\n",
        "        self.shot_num = num_context_shots\r\n",
        "        self.channel = num_out_channels\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\r\n",
        "                               out_channels=self.channel,\r\n",
        "                               kernel_size=(self.shot_num // 2, 1))\r\n",
        "        self.bn = nn.BatchNorm2d(num_features=num_out_channels)\r\n",
        "\r\n",
        "    def forward(self, x):  # [batch_size, seq_len + padding, feat_dim]\r\n",
        "        x = x.unsqueeze(dim=1)\r\n",
        "\r\n",
        "        # batch_size, num_out_channels, seq_len, feat_dim\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = F.relu(self.bn(x))\r\n",
        "        x = x.permute(0, 2, 1, -1)\r\n",
        "        part1 = x[:, :-self.shot_num // 2, :, :]\r\n",
        "        part2 = x[:, self.shot_num // 2:, :, :]\r\n",
        "        part1 = part1.reshape(-1, self.channel, part1.shape[-1])\r\n",
        "        part2 = part2.reshape(-1, self.channel, part2.shape[-1])\r\n",
        "\r\n",
        "        # batch_size, num_out_channels\r\n",
        "        x = F.cosine_similarity(part1.squeeze(),\r\n",
        "                                part2.squeeze(), dim=2)\r\n",
        "        return(x)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJUUSukuYQPG"
      },
      "source": [
        "class BNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(BNet, self).__init__()\r\n",
        "        self.shot_num = num_context_shots\r\n",
        "        self.channel = num_out_channels\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\r\n",
        "                               out_channels=self.channel,\r\n",
        "                               kernel_size=(num_context_shots, 1))\r\n",
        "        self.bn = nn.BatchNorm2d(num_features=num_out_channels)\r\n",
        "        self.max3d = nn.MaxPool3d(kernel_size=(1, self.channel, 1))\r\n",
        "        self.cos = Cos()\r\n",
        "\r\n",
        "    def forward(self, x):  # [batch_size, seq_len + num_context_shots - 1, feat_dim]\r\n",
        "        context = x.unsqueeze(dim=1)\r\n",
        "\r\n",
        "        # batch_size, num_out_channels, seq_len, feat_dim\r\n",
        "        context = F.relu(self.bn(self.conv1(context)))\r\n",
        "\r\n",
        "        # batch_size, seq_len, num_out_channels, feat_dim\r\n",
        "        context = context.permute(0, 2, 1, -1)\r\n",
        "\r\n",
        "        # batch_size,seq_len,1,1,feat_dim\r\n",
        "        context = self.max3d(context.unsqueeze(dim=2))\r\n",
        "\r\n",
        "        # batch_size * seq_len, feat_dim\r\n",
        "        context = context.squeeze().reshape(-1, context.shape[-1])\r\n",
        "\r\n",
        "        # batch_size * seq_len, num_out_channels\r\n",
        "        sim = self.cos(x)\r\n",
        "\r\n",
        "        # batch_size * seq_len, feat_dim + num_out_channels\r\n",
        "        cat_feat = torch.cat((context, sim), dim=1)\r\n",
        "\r\n",
        "        return(cat_feat)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zwr2ggIWybS"
      },
      "source": [
        "class LGSSone(nn.Module):\r\n",
        "    def __init__(self, mode=\"place\"):\r\n",
        "        super(LGSSone, self).__init__()\r\n",
        "        self.seq_len = sequence_length\r\n",
        "        self.num_layers = 2\r\n",
        "        self.lstm_hidden_size = 512\r\n",
        "        self.bidirectional = True\r\n",
        "\r\n",
        "        if mode == \"place\":\r\n",
        "            self.input_dim = (place_feat_dim + num_out_channels)\r\n",
        "            self.bnet = BNet()\r\n",
        "        elif mode == \"cast\":\r\n",
        "            self.bnet = BNet()\r\n",
        "            self.input_dim = (other_feat_dim + num_out_channels)\r\n",
        "        elif mode == \"act\":\r\n",
        "            self.bnet = BNet()\r\n",
        "            self.input_dim = (other_feat_dim + num_out_channels)\r\n",
        "        elif mode == \"aud\":\r\n",
        "            self.bnet = BNet()\r\n",
        "            self.input_dim = (other_feat_dim + num_out_channels)\r\n",
        "        else:\r\n",
        "            pass\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(p=0.2)\r\n",
        "        self.lstm = nn.LSTM(input_size=self.input_dim,\r\n",
        "                            hidden_size=self.lstm_hidden_size,\r\n",
        "                            num_layers=self.num_layers,\r\n",
        "                            batch_first=True,\r\n",
        "                            bidirectional=self.bidirectional)\r\n",
        "\r\n",
        "        if self.bidirectional:\r\n",
        "            self.fc1 = nn.Linear(in_features=self.lstm_hidden_size * 2, \r\n",
        "                                 out_features=100)\r\n",
        "        else:\r\n",
        "            self.fc1 = nn.Linear(in_features=self.lstm_hidden_size, \r\n",
        "                                 out_features=100)\r\n",
        "\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.bnet(x)\r\n",
        "        x = x.reshape(-1, self.seq_len, x.shape[-1])\r\n",
        "        self.lstm.flatten_parameters()\r\n",
        "        out, (_, _) = self.lstm(x, None)\r\n",
        "\r\n",
        "        out = F.relu(self.dropout(self.fc1(out)))\r\n",
        "        return out"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6OqZsoeWyd_"
      },
      "source": [
        "class LGSS(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(LGSS, self).__init__()\r\n",
        "        self.seq_len = sequence_length\r\n",
        "        self.mode = ['place', 'cast', 'act', 'aud']\r\n",
        "        if 'place' in self.mode:\r\n",
        "            self.bnet_place = LGSSone(\"place\")\r\n",
        "            self.instancenorm = nn.InstanceNorm1d(new_sequence_length)\r\n",
        "        if 'cast' in self.mode:\r\n",
        "            self.bnet_cast = LGSSone(\"cast\")\r\n",
        "        if 'act' in self.mode:\r\n",
        "            self.bnet_act = LGSSone(\"act\")\r\n",
        "        if 'aud' in self.mode:\r\n",
        "            self.bnet_aud = LGSSone(\"aud\")\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(in_features=4 * 100, \r\n",
        "                             out_features=100)\r\n",
        "        \r\n",
        "        self.fc2 = nn.Linear(in_features=100, \r\n",
        "                             out_features=2)\r\n",
        "\r\n",
        "\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\r\n",
        "                                        nonlinearity='relu')\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, inp):\r\n",
        "        \r\n",
        "        place_feat = inp[:, :, :place_feat_dim]\r\n",
        "\r\n",
        "        inp = inp[:, :, place_feat_dim:].reshape(-1, new_sequence_length, 3, other_feat_dim)\r\n",
        "        cast_feat, act_feat, aud_feat = torch.split(inp, split_size_or_sections=1, dim=-2)\r\n",
        "\r\n",
        "        cast_feat = cast_feat.squeeze(dim=-2)\r\n",
        "        act_feat = act_feat.squeeze(dim=-2)\r\n",
        "        aud_feat = aud_feat.squeeze(dim=-2)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            place_feat = self.instancenorm(place_feat)\r\n",
        "            cast_feat = self.instancenorm(cast_feat)\r\n",
        "            act_feat = self.instancenorm(act_feat)\r\n",
        "            aud_feat = self.instancenorm(aud_feat)\r\n",
        "\r\n",
        "        place_bound = self.bnet_place(place_feat)\r\n",
        "        cast_bound = self.bnet_cast(cast_feat)\r\n",
        "        act_bound = self.bnet_act(act_feat)\r\n",
        "        aud_bound = self.bnet_aud(aud_feat)\r\n",
        "        \r\n",
        "        final_out = torch.cat((place_bound, cast_bound, act_bound, aud_bound), dim=-1)\r\n",
        "\r\n",
        "        final_out = F.relu(self.fc1(final_out))\r\n",
        "        final_out = self.fc2(final_out).reshape(-1, 2)\r\n",
        "\r\n",
        "        return final_out"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X6fdOhjaWcS"
      },
      "source": [
        "model = LGSS().cuda()\r\n",
        "\r\n",
        "weights = [0.0768, 0.4]\r\n",
        "class_weights = torch.FloatTensor(weights).cuda()\r\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\r\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=2e-1)"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvC2EhLvsz-o"
      },
      "source": [
        "Helper Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqz2BdikMEYK"
      },
      "source": [
        "# Calculates Accuracy\r\n",
        "def get_num_correct(preds, labels):\r\n",
        "    return((preds.argmax(dim=1).eq(labels)).sum().item(), len(labels))"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-dXNYfs3hZ"
      },
      "source": [
        "# Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtwQXhSs3A_"
      },
      "source": [
        "def val_loop():\r\n",
        "    print(\"Validation...\")\r\n",
        "    with torch.no_grad():\r\n",
        "        model.eval()\r\n",
        "        total_loss = 0.0\r\n",
        "        total_correct = 0\r\n",
        "        total_samples = 0\r\n",
        "        for e, (inp, label) in enumerate(val_loader):\r\n",
        "            inp = inp.squeeze(0).cuda()\r\n",
        "            label = label.squeeze().cuda()\r\n",
        "            pred = model(inp)\r\n",
        "\r\n",
        "            loss = criterion(pred, label)\r\n",
        "\r\n",
        "            total_loss += float(loss.item())\r\n",
        "            temp = get_num_correct(pred, label)\r\n",
        "            total_correct += temp[0]\r\n",
        "            total_samples += temp[1]\r\n",
        "            del loss\r\n",
        "            torch.cuda.empty_cache()\r\n",
        "        \r\n",
        "        total_loss = total_loss / len(val_loader)\r\n",
        "        val_acc = 100 * (total_correct / total_samples)\r\n",
        "\r\n",
        "        print(\r\n",
        "            f\"Vaidation Stats:\\n\"\r\n",
        "            f\"Accuracy: {val_acc:.2f}%\",\r\n",
        "            f\"Average Loss Per Batch: {total_loss}\"\r\n",
        "        )"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSm6bZv-JOPo"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqpy3YbEJH5Z"
      },
      "source": [
        "def train_loop(num_epochs):\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        model.train()\r\n",
        "        total_loss = 0.0\r\n",
        "        total_correct = 0\r\n",
        "        total_samples = 0\r\n",
        "        for e, (inp, label) in enumerate(train_loader):\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            inp = inp.squeeze(0).cuda()\r\n",
        "            label = label.squeeze().cuda()\r\n",
        "            pred = model(inp)\r\n",
        "            loss = criterion(pred, label)\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                total_loss += float(loss.item())\r\n",
        "                temp = get_num_correct(pred, label)\r\n",
        "                total_correct += temp[0]\r\n",
        "                total_samples += temp[1]\r\n",
        "            del loss\r\n",
        "            torch.cuda.empty_cache()\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        torch.save(model.state_dict(),\r\n",
        "                '/content/drive/MyDrive/eluvio_data/saved_models/model' + str(epoch + 1) + '.pt')\r\n",
        "        \r\n",
        "        val_loop()\r\n",
        "        \r\n",
        "        total_loss = total_loss / len(train_loader)\r\n",
        "        train_acc = 100 * (total_correct / total_samples)\r\n",
        "\r\n",
        "        print(\r\n",
        "            f\"Training Stats:\\n\"\r\n",
        "            f\"Accuracy: {train_acc:.2f}%\",\r\n",
        "            f\"Average Loss Per Batch: {total_loss}\"\r\n",
        "        )\r\n"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcHYsWFXGZjX",
        "outputId": "0fe1a6ba-8227-404f-c298-3f18bcde9eb9"
      },
      "source": [
        "if load_pretrained_model is False:\r\n",
        "    print('Training...')\r\n",
        "    train_loop(epochs)\r\n",
        "else:\r\n",
        "    print(\"Loading Pretrained Model\")\r\n",
        "    model.load_state_dict(torch.load(model_path))\r\n",
        "    print(\"Pretrained Model Loaded\")"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Pretrained Model\n",
            "Pretrained Model Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVAbngl6GJYB"
      },
      "source": [
        "cnt1 = 0\r\n",
        "cnt2 = 0\r\n",
        "cnt3 = 0\r\n",
        "cnt4 = 0\r\n",
        "with torch.no_grad():\r\n",
        "    model.eval()\r\n",
        "    for e, (inp, label) in enumerate(val_loader):\r\n",
        "\r\n",
        "        inp = inp.squeeze(0).cuda()\r\n",
        "        label = label.squeeze().cuda()\r\n",
        "\r\n",
        "        pred = model(inp)\r\n",
        "        for i, j in zip(label, pred):\r\n",
        "            if i.item() == 1:\r\n",
        "                cnt1 += 1\r\n",
        "                if F.softmax(j, dim=0).argmax().item() == 1:\r\n",
        "                    cnt2 += 1\r\n",
        "            if F.softmax(j, dim=0).argmax().item() == 1:\r\n",
        "                cnt3 += 1\r\n",
        "\r\n",
        "                if i.item() == 1:\r\n",
        "                    cnt4 += 1\r\n",
        "            #print(i.item(), F.softmax(j, dim=0))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvKrEiRjp4Cm"
      },
      "source": [
        "# Test Set Prediction and Dumping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pC-1ifrZ4GI"
      },
      "source": [
        "def dump_predictions(scene_transition_pred):\r\n",
        "    for e, file in enumerate(sorted(glob.glob(data_path + '/*.pkl'))):\r\n",
        "        if e >= 57:\r\n",
        "            new_dict = {}\r\n",
        "            with open(f'{file}', 'rb') as f:\r\n",
        "                data = pickle.load(f)\r\n",
        "            new_dict['imdb_id'] = data['imdb_id']\r\n",
        "            new_dict['scene_transition_boundary_ground_truth'] = data['scene_transition_boundary_ground_truth'].cpu().numpy()\r\n",
        "            new_dict['shot_end_frame'] = data['shot_end_frame'].cpu().numpy()\r\n",
        "            new_dict['scene_transition_boundary_prediction'] = scene_transition_pred[e - 57][:-1].cpu().numpy()\r\n",
        "            assert(len(new_dict['scene_transition_boundary_prediction']) == len(data['scene_transition_boundary_prediction']))\r\n",
        "\r\n",
        "            f_name = file.split('/')[-1]\r\n",
        "            with open(dump_path + f'{f_name}', 'wb') as handle:\r\n",
        "                pickle.dump(new_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ls1OKqYsye"
      },
      "source": [
        "def unpad_predictions(predictions, movie_lengths):    \r\n",
        "    start_idx = 0\r\n",
        "    prediction_list = []\r\n",
        "    for e, movie_len in enumerate(movie_lengths):\r\n",
        "        if movie_len % sequence_length != 0:\r\n",
        "            prediction_list.append(torch.cat(predictions[start_idx: start_idx + movie_len//sequence_length + 1])[:movie_len])\r\n",
        "            start_idx += movie_len//sequence_length + 1\r\n",
        "        else:\r\n",
        "            prediction_list.append(torch.cat(predictions[start_idx: start_idx + movie_len//sequence_length])[:movie_len])\r\n",
        "            start_idx += movie_len//sequence_length \r\n",
        "        \r\n",
        "    return(prediction_list)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKKGXY8FWB6h"
      },
      "source": [
        "def test_loop():\r\n",
        "    scene_transition_pred = []\r\n",
        "    with torch.no_grad():\r\n",
        "        model.eval()\r\n",
        "        for e, (inp, label) in enumerate(test_loader):\r\n",
        "            inp = inp.squeeze(0).cuda()\r\n",
        "            label = label.squeeze().cuda()\r\n",
        "\r\n",
        "            pred = model(inp)\r\n",
        "\r\n",
        "            scene_transition_prob = F.softmax(pred, dim=1)[:, 1]\r\n",
        "            scene_transition_pred.append(scene_transition_prob)\r\n",
        "    return(scene_transition_pred)\r\n"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L48RHj9UKegb"
      },
      "source": [
        "scene_transition_pred = unpad_predictions(test_loop(), test_movie_lengths)\r\n",
        "dump_predictions(scene_transition_pred)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL81UdZglvoX",
        "outputId": "8574b83e-3b66-4eaa-a212-d564060fcaa8"
      },
      "source": [
        "%run '/content/drive/MyDrive/Colab Notebooks/eval.ipynb'"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n",
            "# of IMDB IDs: 7\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt1205489.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt1375666.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt1412386.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt1707386.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt2024544.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt2488496.pkl\n",
            "/content/drive/MyDrive/eluvio_data/data_pred/tt2582846.pkl\n",
            "Scores: {\n",
            "    \"AP\": 0.519881324814132,\n",
            "    \"mAP\": 0.5327515530520018,\n",
            "    \"Miou\": 0.528036302457737,\n",
            "    \"Precision\": 0.4403790682782162,\n",
            "    \"Recall\": 0.6041040503907537,\n",
            "    \"F1\": 0.5020978697516447\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAOZfw0rlvmT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}